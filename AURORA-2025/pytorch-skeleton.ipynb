{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\n\n# Define transformations for the dataset\ntransform = transforms.Compose([\n    transforms.Grayscale(),  # Ensure images are grayscale\n    transforms.ToTensor(),   # Convert to tensor\n    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean and std\n])\n\n# Load training and testing datasets\ntrain_dataset = datasets.EMNIST(root='./data', split='letters', train=True, download=True, transform=transform)\ntest_dataset = datasets.EMNIST(root='./data', split='letters', train=False, download=True, transform=transform)\n\n# DataLoaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:15:00.188487Z","iopub.execute_input":"2025-01-14T11:15:00.188736Z","iopub.status.idle":"2025-01-14T11:15:49.789822Z","shell.execute_reply.started":"2025-01-14T11:15:00.188709Z","shell.execute_reply":"2025-01-14T11:15:49.788705Z"}},"outputs":[{"name":"stdout","text":"Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 561753746/561753746 [00:27<00:00, 20762719.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# PyTorch Docs\n\nConvolution -> https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n\nMax Pool -> https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html\n\nReLU (Rectified Linear Unit) -> https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html\n\nFully Connected (FC Linear Layer) -> https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n\nFlatten has been done for you, do it before you start putting FC layers","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        \n        # # Convolutional layers\n        # self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n\n        # # Fully connected layers\n        # self.fc1 = nn.Linear(64 * 3 * 3, 128)  # Flattened size from conv3\n\n    def forward(self, x):\n        # # Convolutional layers with ReLU and MaxPooling\n        # x = F.relu(F.max_pool2d(self.conv1(x), 2))  # Output: (16, 14, 14)\n\n        # Flatten\n        x = x.view(x.size(0), -1)  # Shape: (batch_size, 64*3*3)\n\n        # # Fully connected layers\n        # x = F.relu(self.fc1(x))\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:31:42.555833Z","iopub.execute_input":"2025-01-14T11:31:42.556194Z","iopub.status.idle":"2025-01-14T11:31:42.562908Z","shell.execute_reply.started":"2025-01-14T11:31:42.556162Z","shell.execute_reply":"2025-01-14T11:31:42.562041Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model = SimpleCNN()\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total Parameters: {total_params}\")\n# Keep it below 150k params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:31:43.456734Z","iopub.execute_input":"2025-01-14T11:31:43.457081Z","iopub.status.idle":"2025-01-14T11:31:43.464059Z","shell.execute_reply.started":"2025-01-14T11:31:43.457053Z","shell.execute_reply":"2025-01-14T11:31:43.463312Z"}},"outputs":[{"name":"stdout","text":"Total Parameters: 100635\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch.optim as optim\n\n# Don't worry about this stuff, its outside our scope. If you want to know more, ask one of us\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:31:48.155173Z","iopub.execute_input":"2025-01-14T11:31:48.155462Z","iopub.status.idle":"2025-01-14T11:31:48.159940Z","shell.execute_reply.started":"2025-01-14T11:31:48.155440Z","shell.execute_reply":"2025-01-14T11:31:48.159053Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import random_split\n\n# Define the train-validation split so you know that your model is not overfitting\n\ntrain_size = int(0.9 * len(train_dataset))  # 90% for training\nval_size = len(train_dataset) - train_size  # Remaining 10% for validation\n\ntrain_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n\n# DataLoaders for train and validation subsets\ntrain_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Outside scope of workshop for syntax but the process has been explained to you during Gradient Descent\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    \n    # Training loop\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct_train += (predicted == labels).sum().item()\n        total_train += labels.size(0)\n    \n    train_loss = running_loss / len(train_loader)\n    train_accuracy = (correct_train / total_train) * 100\n    \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct_val += (predicted == labels).sum().item()\n            total_val += labels.size(0)\n    \n    val_loss /= len(val_loader)\n    val_accuracy = (correct_val / total_val) * 100\n    \n    # Print results for this epoch\n    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:31:49.930343Z","iopub.execute_input":"2025-01-14T11:31:49.930659Z","iopub.status.idle":"2025-01-14T11:41:55.689663Z","shell.execute_reply.started":"2025-01-14T11:31:49.930623Z","shell.execute_reply":"2025-01-14T11:41:55.688680Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Train Loss: 0.5621, Train Accuracy: 82.38%, Val Loss: 0.2844, Val Accuracy: 90.54%\nEpoch [2/10], Train Loss: 0.2525, Train Accuracy: 91.67%, Val Loss: 0.2406, Val Accuracy: 92.05%\nEpoch [3/10], Train Loss: 0.2083, Train Accuracy: 92.99%, Val Loss: 0.2004, Val Accuracy: 93.29%\nEpoch [4/10], Train Loss: 0.1821, Train Accuracy: 93.74%, Val Loss: 0.1903, Val Accuracy: 93.46%\nEpoch [5/10], Train Loss: 0.1646, Train Accuracy: 94.26%, Val Loss: 0.1832, Val Accuracy: 93.77%\nEpoch [6/10], Train Loss: 0.1504, Train Accuracy: 94.62%, Val Loss: 0.1853, Val Accuracy: 93.93%\nEpoch [7/10], Train Loss: 0.1387, Train Accuracy: 94.96%, Val Loss: 0.1841, Val Accuracy: 93.77%\nEpoch [8/10], Train Loss: 0.1287, Train Accuracy: 95.24%, Val Loss: 0.1890, Val Accuracy: 93.89%\nEpoch [9/10], Train Loss: 0.1191, Train Accuracy: 95.56%, Val Loss: 0.1899, Val Accuracy: 93.87%\nEpoch [10/10], Train Loss: 0.1142, Train Accuracy: 95.64%, Val Loss: 0.1973, Val Accuracy: 93.80%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:45:16.655428Z","iopub.execute_input":"2025-01-14T11:45:16.655756Z","iopub.status.idle":"2025-01-14T11:45:24.243113Z","shell.execute_reply.started":"2025-01-14T11:45:16.655730Z","shell.execute_reply":"2025-01-14T11:45:24.242257Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 93.46%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"torch.save(model.state_dict(), \"simple_cnn_ocr.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T09:12:43.815044Z","iopub.execute_input":"2025-01-01T09:12:43.815467Z","iopub.status.idle":"2025-01-01T09:12:43.822967Z","shell.execute_reply.started":"2025-01-01T09:12:43.815422Z","shell.execute_reply":"2025-01-01T09:12:43.821895Z"}},"outputs":[],"execution_count":8}]}